---
---

@string{aps = {American Physical Society,}}

@article{jatavallabhula2023conceptfusion,
  abbr={RSS},
  title={Conceptfusion: Open-set multimodal 3d mapping},
  author={Jatavallabhula, Krishna Murthy and Kuwajerwala, Alihusein and Gu, Qiao and Omama, Mohd and Chen, Tao and Li, Shuang and Iyer, Ganesh and Saryazdi, Soroush and Keetha, Nikhil and Tewari, Ayush and others},
  journal={arXiv preprint arXiv:2302.07241},
  year={2023},
  selected={true}
}

@INPROCEEDINGS{9981330,
  abbr={IROS},

  author={Omama, Mohd and Sundar, Sripada V. S. and Chinchali, Sandeep and Singh, Arun Kumar and Krishna, K. Madhava},

  booktitle={2022 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)}, 

  title={Drift Reduced Navigation with Deep Explainable Features}, 

  year={2022},

  volume={},

  number={},

  pages={6316-6323},

  doi={10.1109/IROS47612.2022.9981330},
  selected={true}
  }



@inproceedings{omama2022ladfn,
  abbr={ACC},
  title={Ladfn: Learning actions for drift-free navigation in highly dynamic scenes},
  author={Omama, Mohd and VS, Sundar Sripada and Chinchali, Sandeep and Krishna, K Madhava},
  booktitle={2022 American Control Conference (ACC)},
  pages={1200--1207},
  year={2022},
  organization={IEEE}
}

@InProceedings{10.1007/978-3-031-73229-4_13,
abbr={ECCV},
author="Choi, Minkyu
and Goel, Harsh
and Omama, Mohammad
and Yang, Yunhao
and Shah, Sahil
and Chinchali, Sandeep",
editor="Leonardis, Ale{\v{s}}
and Ricci, Elisa
and Roth, Stefan
and Russakovsky, Olga
and Sattler, Torsten
and Varol, G{\"u}l",
title="Towards Neuro-Symbolic Video Understanding",
booktitle="Computer Vision -- ECCV 2024",
year="2024",
publisher="Springer Nature Switzerland",
address="Cham",
pages="220--236",
abstract="The unprecedented surge in video data production in recent years necessitates efficient tools to extract meaningful frames from videos for downstream tasks. Long-term temporal reasoning is a key desideratum for frame retrieval systems. While state-of-the-art foundation models, like VideoLLaMA and ViCLIP, are proficient in short-term semantic understanding, they surprisingly fail at long-term reasoning across frames. A key reason for this failure is that they intertwine per-frame perception and temporal reasoning into a single deep network. Hence, decoupling but co-designing the semantic understanding and temporal reasoning is essential for efficient scene identification. We propose a system that leverages vision-language models for semantic understanding of individual frames and effectively reasons about the long-term evolution of events using state machines and temporal logic (TL) formulae that inherently capture memory. Our TL-based reasoning improves the F1 score of complex event identification by 9--15{\%}, compared to benchmarks that use GPT-4 for reasoning, on state-of-the-art self-driving datasets such as Waymo and NuScenes. The source code is available at https://github.com/UTAustin-SwarmLab/Neuro-Symbolic-Video-Search-Temporal-Logic.",
isbn="978-3-031-73229-4"
}


@article{omama2024exploiting,
  abbr={ICLR},
  title={Exploiting Distribution Constraints for Scalable and Efficient Image Retrieval},
  author={Omama, Mohammad and Li, Po-han and Chinchali, Sandeep P},
  journal={arXiv preprint arXiv:2410.07022},
  year={2025}
}